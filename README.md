<div align="center">

[![Typing SVG](https://readme-typing-svg.demolab.com?font=Josefin+Sans&pause=1000&color=9D00FF&center=true&vCenter=true&width=435&lines=Sign+Language+to+Speech+Conversion;Real-time+ASL+Recognition+System)](https://github.com/RhythmusByte/Sign-Language-to-Speech)

<img src="banner.png" alt="Project demonstration" width="100%"/>

[![License](https://img.shields.io/badge/License-BSD_3--Clause-8A2BE2.svg?style=for-the-badge)](https://opensource.org/licenses/BSD-3-Clause)
![Status](https://img.shields.io/badge/Status-Active_Development-8A2BE2?style=for-the-badge&logo=vercel)

</div>

---

## ðŸŽ¯ Project Overview
**Sign Language to Speech Conversion** is a real-time **American Sign Language (ASL) recognition system** powered by **computer vision** and **deep learning**. It translates ASL hand gestures into **both text and speech output**, enhancing accessibility and communication.

> ðŸ”® **Development Update:** Core functionality implemented â€“ refining accuracy and expanding the gesture library.

---

## âœ¨ Key Features
- ðŸ”® **Real-time** hand detection & gesture tracking  
- ðŸ§  **CNN-based** classification using TensorFlow/Keras  
- ðŸ”Š Simultaneous **text & speech** output  
- ðŸ“¢ Designed for **accessibility & inclusivity**  

---

## ðŸ“– Full Documentation  

For **detailed setup instructions, architecture, installation guide, and troubleshooting**, visit the [Project Wiki](https://github.com/RhythmusByte/Sign-Language-to-Speech/wiki).  

---

## ðŸ“Š System Architecture  

| Level 0 | Level 1 | Level 2 |
|---------|---------|---------|
| ![DFD Level 0](DFD_0.png) | ![DFD Level 1](DFD_1.png) | ![DFD Level 2](DFD_2.png) |

For a full breakdown of the **Data Flow Diagrams (DFD), Use Case Diagrams, and System Design**, refer to the [Architecture Section](https://github.com/RhythmusByte/Sign-Language-to-Speech/wiki/System-Architecture-and-Design) in the Wiki.

---

## ðŸ›  Tech Stack  

### **Core Technologies**
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![OpenCV](https://img.shields.io/badge/OpenCV-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white)
![TensorFlow](https://img.shields.io/badge/TensorFlow-9D00FF?style=for-the-badge&logo=tensorflow&logoColor=white)

### **Supporting Libraries**
![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)
![cvzone](https://img.shields.io/badge/cvzone-Community-9D00FF?style=for-the-badge)
![pyttsx3](https://img.shields.io/badge/pyttsx3-TTS_Engine-8B0000?style=for-the-badge)

---

## ðŸ“‚ Repository Structure  

```text
Sign-Language-to-Speech/
â”œâ”€â”€ data/                 
â”œâ”€â”€ Application.py        
â”œâ”€â”€ trainedModel.h5       
â”œâ”€â”€ requirements.txt      
â””â”€â”€ white.jpg             
```

For a **detailed breakdown of modules and system design**, refer to the [Project Documentation](https://github.com/RhythmusByte/Sign-Language-to-Speech/wiki).

---

## ðŸ“¢ Contributing  

We welcome contributions! Before submitting a pull request, please check out the **[Contributing Guide](https://github.com/RhythmusByte/Sign-Language-to-Speech/wiki/Contributing)** in the Wiki.

---

## ðŸ“œ License  

This project is licensed under the **BSD 3-Clause License**. See the full details in the [LICENSE](LICENSE) file.

---

ðŸ“Œ **Go to the Wiki:**  
ðŸ“– **[Sign Language to Speech Wiki](https://github.com/RhythmusByte/Sign-Language-to-Speech/wiki)**  
