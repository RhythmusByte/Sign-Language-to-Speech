<div align="center">

[![Typing SVG](https://readme-typing-svg.demolab.com?font=Josefin+Sans&pause=1000&color=9D00FF&center=true&vCenter=true&width=435&lines=Sign+Language+to+Speech+Conversion;Real-time+ASL+Recognition+System)](https://github.com/RhythmusByte/Sign-Language-to-Speech)

<img src="banner.png" alt="Project demonstration" width="100%"/>

[![License](https://img.shields.io/badge/License-BSD_3--Clause-8A2BE2.svg?style=for-the-badge)](https://opensource.org/licenses/BSD-3-Clause)
![Status](https://img.shields.io/badge/Status-Halted_Development-8A2BE2?style=for-the-badge&logo=vercel)

</div>

---

> <!--HALTED-DAYS-->ðŸ”® Project Status: Development Temporarily Suspended â€“ 0 Days Elapsed<!--HALTED-DAYS-->

## ðŸŽ¯ Project Overview  
**Sign Language to Speech Conversion** is a real-time **American Sign Language (ASL) recognition system** powered by **computer vision** and **deep learning**. It translates ASL hand gestures into **both text and speech output**, enhancing accessibility and communication.  

ðŸ“– For installation, architecture, usage, and contribution guidelines, visit the **[Project Wiki](https://github.com/RhythmusByte/Sign-Language-to-Speech/wiki)**.  

---

## âœ¨ Key Features  
- ðŸ”® **Real-time** hand detection & gesture tracking  
- ðŸ§  **CNN-based** classification using TensorFlow/Keras  
- ðŸ”Š Simultaneous **text & speech** output  
- ðŸ“¢ Designed for **accessibility & inclusivity**  

---

## ðŸ“Š System Architecture  

| Level 0 | Level 1 | Level 2 |
|---------|---------|---------|
| ![DFD Level 0](DFD_0.png) | ![DFD Level 1](DFD_1.png) | ![DFD Level 2](DFD_2.png) |

For details on **Data Flow Diagrams (DFD), Use Case Diagrams, and System Design**, check the **[Architecture Section](https://github.com/RhythmusByte/Sign-Language-to-Speech/wiki/System-Architecture-&-Design)** in the Wiki.  

---

## ðŸ›  Tech Stack  

### **Core Technologies**  
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)  
![OpenCV](https://img.shields.io/badge/OpenCV-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white)  
![TensorFlow](https://img.shields.io/badge/TensorFlow-9D00FF?style=for-the-badge&logo=tensorflow&logoColor=white)  

### **Supporting Libraries**  
![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)  
![cvzone](https://img.shields.io/badge/cvzone-Community-9D00FF?style=for-the-badge)  
![pyttsx3](https://img.shields.io/badge/pyttsx3-TTS_Engine-8B0000?style=for-the-badge)  

---

## ðŸ“‚ Repository Structure  

```text
Sign-Language-to-Speech/
â”œâ”€â”€ data/            
â”œâ”€â”€ Application.py   
â”œâ”€â”€ trainedModel.h5  
â”œâ”€â”€ requirements.txt 
â””â”€â”€ white.jpg        
```

For a **detailed breakdown of modules and system design**, refer to the **[Project Documentation](https://github.com/RhythmusByte/Sign-Language-to-Speech/wiki)**.  

---

## ðŸ“¢ Contributing  

We welcome contributions! Before submitting a pull request, please check out the **[Contributing Guide](https://github.com/RhythmusByte/Sign-Language-to-Speech/wiki/Contributions)**.  

---

## ðŸ“œ License  

This project is licensed under the **BSD 3-Clause License**. See the full details in the [LICENSE](LICENSE) file.  

---

ðŸ“Œ **For all documentation, including installation, setup, and FAQs, visit the** ðŸ‘‰ **[Project Wiki](https://github.com/RhythmusByte/Sign-Language-to-Speech/wiki)**.  
