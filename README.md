<div align="center">

[![Typing SVG](https://readme-typing-svg.demolab.com?font=Josefin+Sans&pause=1000&color=FF6F00&center=true&vCenter=true&width=435&lines=Sign+Language+to+Speech+Conversion;Real-time+ASL+Recognition+System)](https://github.com/RhythmusByte/Sign-Language-to-Speech)

<!-- <img src="" alt="Project demonstration" height="200" /> -->

[![License](https://img.shields.io/badge/License-BSD_3--Clause-ffd700.svg?style=for-the-badge)](https://opensource.org/licenses/BSD-3-Clause)
![Status](https://img.shields.io/badge/Status-Active_Development-important?style=for-the-badge&logo=vercel)

</div>

---

## ğŸ¯ Project Overview
Real-time American Sign Language (ASL) translation system using computer vision and deep learning. Combines custom hand tracking with gesture classification to create accessible communication solutions.

> âš ï¸ **Development Notice:** Core functionality implemented - refining accuracy and expanding gesture library

---

## âœ¨ Key Features
- ğŸ– Real-time hand detection & gesture tracking
- ğŸ§  CNN-based classification using TensorFlow/Keras
- ğŸ”Š Simultaneous text & speech output

---

## ğŸ›  Tech Stack
### **Core Technologies**
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![OpenCV](https://img.shields.io/badge/OpenCV-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white)
![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)

### **Supporting Libraries**
![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)
![cvzone](https://img.shields.io/badge/cvzone-Community-9cf?style=for-the-badge)
![pyttsx3](https://img.shields.io/badge/pyttsx3-TTS_Engine-8B0000?style=for-the-badge)

---

## ğŸ“‚ Repository Structure
```text
Sign-Language-to-Speech/
â”œâ”€â”€ data/                  # Training datasets and gesture samples
â”œâ”€â”€ Application.py        # Main application logic
â”œâ”€â”€ trainedModel.h5       # Pretrained CNN model
â”œâ”€â”€ requirements.txt      # Dependency specifications
â””â”€â”€ white.jpg             # Background reference image
```

---

## ğŸš€ Installation Guide

### **Prerequisites**
- Python 3.9+ (Recommended)
- Webcam-enabled device
- 4GB+ RAM recommended

### **Setup Instructions**
1. Clone repository:
   ```bash
   git clone https://github.com/RhythmusByte/Sign-Language-to-Speech.git
   cd Sign-Language-to-Speech
   ```

2. Create virtual environment:
   ```bash
   python -m venv .venv
   ```

3. Activate environment:
   - Windows: `.venv\Scripts\activate`
   - Unix/MacOS: `source .venv/bin/activate`

4. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ–¥ Usage
Launch the recognition system:
```bash
python Application.py
```

**System Flow:**
1. Webcam initialization
2. Hand detection & ROI extraction
3. Gesture preprocessing & normalization
4. CNN-based classification
5. Real-time text & speech output

---

## ğŸ§­ Development Roadmap
- [ ] Expand ASL gesture library (A-Z completion)
- [ ] Implement user-customizable gestures
- [ ] Optimize for low-power devices
- [ ] Add multi-hand detection capability
- [ ] Develop training interface for model updates

---

## ğŸ“œ License
Distributed under BSD 3-Clause License. See `LICENSE` for full text.

---

<div align="center">

[â¬† Back to Top](#sign-language-to-speech-conversion)

</div>
